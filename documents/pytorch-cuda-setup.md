# PyTorch CUDA ç¯å¢ƒé…ç½®æ€»ç»“

æœ¬æ–‡æ¡£æ€»ç»“äº†åœ¨ NixOS/Nix ç¯å¢ƒä¸­é…ç½® PyTorch CUDA æ”¯æŒçš„å®Œæ•´æ–¹æ³•ï¼Œé€‚ç”¨äºæœºå™¨å­¦ä¹ é¡¹ç›®å¼€å‘ç¯å¢ƒã€‚

## ğŸ¯ é…ç½®ç›®æ ‡

- âœ… Python 3.12 å¼€å‘ç¯å¢ƒ
- âœ… PyTorch 2.5.1+cu124 (CUDA 12.4 æ”¯æŒ)
- âœ… å®Œæ•´çš„æœºå™¨å­¦ä¹ å·¥å…·é“¾
- âœ… æ— éœ€ç³»ç»Ÿçº§ä¿®æ”¹çš„ CUDA ç¯å¢ƒ
- âœ… æœ¬åœ°ç¼–è¯‘ä¼˜åŒ–æ€§èƒ½ï¼ˆæ¨èï¼‰

## ğŸ“ é¡¹ç›®ç»“æ„

```
MarketDataProvider/
â”œâ”€â”€ flake.nix                 # ä¸»é…ç½®æ–‡ä»¶
â”œâ”€â”€ flake.lock               # é”å®šç‰ˆæœ¬æ–‡ä»¶
â”œâ”€â”€ nix/
â”‚   â”œâ”€â”€ development.nix      # å¼€å‘ç¯å¢ƒé…ç½®
â”‚   â”œâ”€â”€ packages.nix         # åŒ…é…ç½®
â”‚   â””â”€â”€ README.md           # ä½¿ç”¨è¯´æ˜
â””â”€â”€ docs/
    â””â”€â”€ pytorch-cuda-setup.md  # æœ¬æ–‡æ¡£
```

## ğŸ”§ å…³é”®é…ç½®è¦ç‚¹

### 1. ç¼–è¯‘æ–¹å¼é€‰æ‹©

#### æ–¹æ¡ˆ A: æœ¬åœ°ç¼–è¯‘ï¼ˆæ¨è - æ›´ä¼˜æ€§èƒ½ï¼‰

**ä¼˜åŠ¿**: 
- é’ˆå¯¹æœ¬æœº CPU/GPU ä¼˜åŒ–ç¼–è¯‘ï¼Œæ€§èƒ½æ›´ä½³
- å¯ä»¥å¯ç”¨ç‰¹å®šçš„ä¼˜åŒ–é€‰é¡¹
- è·å¾—æœ€æ–°çš„åŠŸèƒ½å’Œä¿®å¤

**åŠ£åŠ¿**:
- ç¼–è¯‘æ—¶é—´é•¿ï¼ˆ1-3å°æ—¶ï¼‰
- éœ€è¦è¶³å¤Ÿå†…å­˜å’Œå­˜å‚¨ç©ºé—´
- ç¼–è¯‘å¯èƒ½å¤±è´¥ï¼Œéœ€è¦å¤šæ¬¡å°è¯•

```nix
# âœ… æœ¬åœ°ç¼–è¯‘æ–¹å¼ï¼ˆæ¨èï¼‰
torch          # ä»æºç ç¼–è¯‘çš„ CUDA PyTorch
torchvision    # ä»æºç ç¼–è¯‘çš„ CUDA torchvision  
torchaudio     # ä»æºç ç¼–è¯‘çš„ CUDA torchaudio
```

#### æ–¹æ¡ˆ B: é¢„ç¼–è¯‘äºŒè¿›åˆ¶ï¼ˆæ€§èƒ½è¾ƒå¼±æœºå™¨æ¨èï¼‰

**ä¼˜åŠ¿**: 
- å¿«é€Ÿå®‰è£…ï¼Œæ— éœ€ç¼–è¯‘
- ç¨³å®šå¯é 
- é€‚åˆæ€§èƒ½è¾ƒå¼±çš„æœºå™¨

**åŠ£åŠ¿**:
- æ€§èƒ½ç›¸å¯¹è¾ƒä½
- æ— æ³•é’ˆå¯¹ç‰¹å®šç¡¬ä»¶ä¼˜åŒ–

```nix
# âœ… é¢„ç¼–è¯‘æ–¹å¼ï¼ˆæ€§èƒ½è¾ƒå¼±æœºå™¨æ¨èï¼‰
torch-bin      # é¢„ç¼–è¯‘çš„ CUDA PyTorch
torchvision-bin # é¢„ç¼–è¯‘çš„ CUDA torchvision  
torchaudio-bin  # é¢„ç¼–è¯‘çš„ CUDA torchaudio
```

### 2. å…¨å±€å¯ç”¨ CUDA æ”¯æŒ

åœ¨ `development.nix` ä¸­ï¼š

```nix
_module.args.pkgs = import inputs.nixpkgs {
  inherit system;
  config = {
    allowUnfree = true;
    cudaSupport = true;  # ğŸ”‘ å…³é”®é…ç½®
  };
};
```

### 3. åŒ…å«å¿…è¦çš„ CUDA ä¾èµ–åŒ…

```nix
packages = with pkgs; [
  # CUDA dependencies - å…³é”®åŒ…
  cudatoolkit
  linuxPackages.nvidia_x11
  libGLU
  libGL
  xorg.libXi
  xorg.libXmu
  freeglut
  xorg.libXext
  xorg.libX11
  xorg.libXv
  xorg.libXrandr
  ncurses5
  stdenv.cc
  binutils
];
```

### 4. è®¾ç½® CUDA ç¯å¢ƒå˜é‡

åœ¨ `shellHook` ä¸­ï¼š

```bash
# CUDA environment setup (no system modification needed)
export CUDA_PATH=${pkgs.cudatoolkit}
export LD_LIBRARY_PATH=${pkgs.linuxPackages.nvidia_x11}/lib:${pkgs.ncurses5}/lib:$LD_LIBRARY_PATH
export EXTRA_LDFLAGS="-L${pkgs.linuxPackages.nvidia_x11}/lib"
export EXTRA_CCFLAGS="-I${pkgs.cudatoolkit}/include"
```

## ğŸ“‹ å®Œæ•´çš„å¼€å‘ç¯å¢ƒåŒ…åˆ—è¡¨

### æ ¸å¿ƒæœºå™¨å­¦ä¹ åŒ…ï¼ˆæœ¬åœ°ç¼–è¯‘ç‰ˆæœ¬ï¼‰
```nix
# Core data processing
pandas
numpy
scipy

# Machine learning and scientific computing
scikit-learn
numba
lightgbm
torch          # ä»æºç ç¼–è¯‘çš„ CUDA PyTorchï¼ˆæ¨èï¼‰
torchvision    # ä»æºç ç¼–è¯‘çš„ CUDA torchvision
torchaudio     # ä»æºç ç¼–è¯‘çš„ CUDA torchaudio
```

### æ ¸å¿ƒæœºå™¨å­¦ä¹ åŒ…ï¼ˆé¢„ç¼–è¯‘ç‰ˆæœ¬ - æ€§èƒ½è¾ƒå¼±æœºå™¨ï¼‰
```nix
# Core data processing
pandas
numpy
scipy

# Machine learning and scientific computing
scikit-learn
numba
lightgbm
torch-bin      # é¢„ç¼–è¯‘çš„ CUDA PyTorchï¼ˆæ€§èƒ½è¾ƒå¼±æœºå™¨ï¼‰
torchvision-bin # é¢„ç¼–è¯‘çš„ CUDA torchvision
torchaudio-bin  # é¢„ç¼–è¯‘çš„ CUDA torchaudio
```

### å¼€å‘å·¥å…·
```nix
# Development tools
python312Packages.black
python312Packages.flake8
python312Packages.pytest
python312Packages.mypy
python312Packages.ipython
python312Packages.jupyterlab
```

### å…¶ä»–å·¥å…·
```nix
# Market data and APIs
requests
httpx
aiohttp
fastapi
uvicorn

# Database
sqlalchemy
psycopg2
aiomysql
redis

# MQTT
aiomqtt
paho-mqtt
mosquitto

# Data formats
openpyxl
xlsxwriter
```

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### æœ¬åœ°ç¼–è¯‘æ–¹å¼ï¼ˆæ¨èï¼‰

```bash
cd MarketDataProvider

# æ›´æ–° flake ä¾èµ–
nix flake update

# é™åˆ¶å¹¶å‘ç¼–è¯‘ä»¥æé«˜æˆåŠŸç‡
nix develop --option cores 8 --option max-jobs 1
```

**âš ï¸ é‡è¦æç¤º**:
- `--option cores 8`: é™åˆ¶æ¯ä¸ªä»»åŠ¡æœ€å¤šä½¿ç”¨ 8 ä¸ª CPU æ ¸å¿ƒ
- `--option max-jobs 1`: é™åˆ¶åŒæ—¶åªèƒ½è¿è¡Œ 1 ä¸ªç¼–è¯‘ä»»åŠ¡
- ç¼–è¯‘å¯èƒ½éœ€è¦ 1-3 å°æ—¶ï¼Œè¯·è€å¿ƒç­‰å¾…
- **æ³¨æ„**: å¯èƒ½éœ€è¦ç¼–è¯‘å¤šæ¬¡æ‰èƒ½æˆåŠŸï¼å¦‚æœç¼–è¯‘å¤±è´¥ï¼Œè¯·é‡æ–°è¿è¡Œå‘½ä»¤

### é¢„ç¼–è¯‘æ–¹å¼ï¼ˆæ€§èƒ½è¾ƒå¼±æœºå™¨ï¼‰

```bash
cd MarketDataProvider

# ä½¿ç”¨é¢„ç¼–è¯‘ç‰ˆæœ¬ï¼Œå¿«é€Ÿè¿›å…¥å¼€å‘ç¯å¢ƒ
nix develop
```

### éªŒè¯ CUDA çŠ¶æ€
```python
import torch
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"CUDA devices: {torch.cuda.device_count()}")
print(f"Current device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}")
```

### æµ‹è¯• GPU è®¡ç®—
```python
import torch

# åˆ›å»ºå¼ é‡å¹¶ç§»åˆ° GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
x = torch.randn(1000, 1000).to(device)
y = torch.randn(1000, 1000).to(device)

# GPU çŸ©é˜µä¹˜æ³•
result = torch.matmul(x, y)
print(f"è®¡ç®—å®Œæˆï¼Œè®¾å¤‡: {result.device}")
```

## ğŸ” æ•…éšœæ’æŸ¥

### é—®é¢˜ 1: CUDA available: False

**å¯èƒ½åŸå› **:
1. ç¼ºå°‘ NVIDIA é©±åŠ¨åŒ…
2. ç¯å¢ƒå˜é‡æœªæ­£ç¡®è®¾ç½®
3. ä½¿ç”¨äº† CPU ç‰ˆæœ¬çš„ torch

**è§£å†³æ–¹æ¡ˆ**:
1. ç¡®ä¿åŒ…å« `linuxPackages.nvidia_x11`
2. æ£€æŸ¥ `LD_LIBRARY_PATH` è®¾ç½®
3. ä½¿ç”¨ `torch`ï¼ˆæœ¬åœ°ç¼–è¯‘ï¼‰æˆ– `torch-bin`ï¼ˆé¢„ç¼–è¯‘ï¼‰

### é—®é¢˜ 2: ç¼–è¯‘é”™è¯¯æˆ–å¤±è´¥

**å¯èƒ½åŸå› **:
1. å†…å­˜ä¸è¶³
2. ç£ç›˜ç©ºé—´ä¸è¶³
3. å¹¶å‘ä»»åŠ¡è¿‡å¤šå¯¼è‡´èµ„æºç«äº‰
4. ç½‘ç»œé—®é¢˜å¯¼è‡´ä¾èµ–ä¸‹è½½å¤±è´¥

**è§£å†³æ–¹æ¡ˆ**:
1. ä½¿ç”¨ `--option cores 8 --option max-jobs 1` é™åˆ¶å¹¶å‘
2. ç¡®ä¿è‡³å°‘æœ‰ 16GB å¯ç”¨å†…å­˜
3. ç¡®ä¿è‡³å°‘æœ‰ 20GB å¯ç”¨ç£ç›˜ç©ºé—´
4. **å¤šæ¬¡å°è¯•ç¼–è¯‘**ï¼Œæœ‰æ—¶éœ€è¦ 2-3 æ¬¡æ‰èƒ½æˆåŠŸ
5. å¦‚æœå¤šæ¬¡å¤±è´¥ï¼Œåˆ‡æ¢åˆ°é¢„ç¼–è¯‘ç‰ˆæœ¬ï¼ˆ`torch-bin`ï¼‰

### é—®é¢˜ 3: ç¼–è¯‘æ—¶é—´è¿‡é•¿

**è§£å†³æ–¹æ¡ˆ**:
1. æœ¬åœ°ç¼–è¯‘é€šå¸¸éœ€è¦ 1-3 å°æ—¶ï¼Œè¿™æ˜¯æ­£å¸¸çš„
2. å¯ä»¥åœ¨åå°è¿è¡Œï¼š`nohup nix develop --option cores 8 --option max-jobs 1 &`
3. å¦‚æœæ—¶é—´ç´§æ€¥ï¼Œä½¿ç”¨é¢„ç¼–è¯‘ç‰ˆæœ¬ï¼ˆ`torch-bin`ï¼‰

### é—®é¢˜ 4: ç‰ˆæœ¬å†²çª

**å¯èƒ½åŸå› **:
1. æ··ç”¨ä¸åŒæ¥æºçš„ PyTorch åŒ…
2. ä¾èµ–åŒ…ç‰ˆæœ¬ä¸åŒ¹é…

**è§£å†³æ–¹æ¡ˆ**:
1. ç»Ÿä¸€ä½¿ç”¨ nixpkgs çš„ç‰ˆæœ¬ï¼ˆ`torch` æˆ– `torch-bin`ï¼‰
2. ç¡®ä¿ `cudaSupport = true` å…¨å±€è®¾ç½®
3. è¿è¡Œ `nix flake update` æ›´æ–°ä¾èµ–

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### æœ¬åœ°ç¼–è¯‘ vs é¢„ç¼–è¯‘æ€§èƒ½æµ‹è¯•

```python
import torch
import time

def benchmark_performance():
    if not torch.cuda.is_available():
        print("CUDA ä¸å¯ç”¨")
        return
    
    device = torch.device('cuda')
    
    # æµ‹è¯•ä¸åŒçŸ©é˜µå¤§å°
    sizes = [1000, 3000, 5000]
    
    for size in sizes:
        a = torch.randn(size, size, device=device)
        b = torch.randn(size, size, device=device)
        
        # é¢„çƒ­
        torch.matmul(a, b)
        torch.cuda.synchronize()
        
        # åŸºå‡†æµ‹è¯•
        start_time = time.time()
        for _ in range(10):
            result = torch.matmul(a, b)
        torch.cuda.synchronize()
        end_time = time.time()
        
        avg_time = (end_time - start_time) / 10
        print(f"çŸ©é˜µå¤§å° {size}x{size}: {avg_time:.4f}s")

if __name__ == "__main__":
    print(f"PyTorch version: {torch.__version__}")
    print(f"CUDA version: {torch.version.cuda}")
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print("=" * 50)
    benchmark_performance()
```

**é¢„æœŸæ€§èƒ½å·®å¼‚**:
- æœ¬åœ°ç¼–è¯‘ç‰ˆæœ¬é€šå¸¸æ¯”é¢„ç¼–è¯‘ç‰ˆæœ¬å¿« 5-15%
- åœ¨ç‰¹å®šç¡¬ä»¶ä¸Šä¼˜åŒ–æ›´æ˜æ˜¾
- å¯¹äºå¤§è§„æ¨¡è®­ç»ƒä»»åŠ¡ï¼Œæ€§èƒ½æå‡æ›´æ˜¾è‘—

## ğŸ’¡ æœ€ä½³å®è·µå»ºè®®

### é€‰æ‹©ç¼–è¯‘æ–¹å¼çš„å†³ç­–æ ‘

```
ä½ çš„æœºå™¨é…ç½®å¦‚ä½•ï¼Ÿ
â”œâ”€â”€ é«˜æ€§èƒ½ (32GB+ å†…å­˜, 8+ æ ¸å¿ƒ CPU)
â”‚   â”œâ”€â”€ æ—¶é—´å……è£• â†’ é€‰æ‹©æœ¬åœ°ç¼–è¯‘
â”‚   â””â”€â”€ æ—¶é—´ç´§æ€¥ â†’ é€‰æ‹©é¢„ç¼–è¯‘ï¼Œåç»­å†åˆ‡æ¢
â”œâ”€â”€ ä¸­ç­‰æ€§èƒ½ (16GB+ å†…å­˜, 4+ æ ¸å¿ƒ CPU)
â”‚   â”œâ”€â”€ æ„¿æ„ç­‰å¾… â†’ å°è¯•æœ¬åœ°ç¼–è¯‘
â”‚   â””â”€â”€ è¿½æ±‚ç¨³å®š â†’ é€‰æ‹©é¢„ç¼–è¯‘
â””â”€â”€ ä½æ€§èƒ½ (< 16GB å†…å­˜, < 4 æ ¸å¿ƒ CPU)
    â””â”€â”€ å¼ºçƒˆå»ºè®®ä½¿ç”¨é¢„ç¼–è¯‘
```

### æ··åˆç­–ç•¥
1. **å¼€å‘é˜¶æ®µ**: ä½¿ç”¨é¢„ç¼–è¯‘ç‰ˆæœ¬å¿«é€Ÿå¼€å§‹
2. **ä¼˜åŒ–é˜¶æ®µ**: åˆ‡æ¢åˆ°æœ¬åœ°ç¼–è¯‘ç‰ˆæœ¬è·å¾—æ›´å¥½æ€§èƒ½
3. **ç”Ÿäº§éƒ¨ç½²**: æ ¹æ®æœåŠ¡å™¨é…ç½®é€‰æ‹©åˆé€‚ç‰ˆæœ¬

## ğŸ‰ æˆåŠŸæ ‡å¿—

å½“ç¯å¢ƒæ­£ç¡®é…ç½®åï¼Œä½ åº”è¯¥çœ‹åˆ°ï¼š

```
ğŸ”¥ PyTorch CUDA status:
PyTorch: 2.5.1+cu124
CUDA available: True
CUDA devices: 1
```

## ğŸ“š å‚è€ƒèµ„æº

- [NixOS CUDA Wiki](https://nixos.wiki/wiki/CUDA)
- [PyTorch CUDA æ–‡æ¡£](https://pytorch.org/get-started/locally/)
- [nixpkgs CUDA æ–‡æ¡£](https://ryantm.github.io/nixpkgs/languages-frameworks/cuda/)
- [Nix ç¼–è¯‘é€‰é¡¹æ–‡æ¡£](https://nixos.org/manual/nix/stable/command-ref/conf-file.html)

## ğŸ·ï¸ ç‰ˆæœ¬ä¿¡æ¯

- **åˆ›å»ºæ—¥æœŸ**: 2025-01-31
- **æ›´æ–°æ—¥æœŸ**: 2025-01-31
- **NixOS ç‰ˆæœ¬**: 24.11
- **PyTorch ç‰ˆæœ¬**: 2.5.1+cu124
- **CUDA ç‰ˆæœ¬**: 12.4
- **Python ç‰ˆæœ¬**: 3.12.8

---

*æœ¬æ–‡æ¡£åŸºäº MarketDataProvider é¡¹ç›®çš„å®é™…é…ç½®ç»éªŒç¼–å†™ï¼Œå·²åœ¨ NVIDIA GeForce RTX 4070 Ti SUPER ä¸ŠéªŒè¯æˆåŠŸã€‚åŒ…å«æœ¬åœ°ç¼–è¯‘å’Œé¢„ç¼–è¯‘ä¸¤ç§æ–¹æ¡ˆï¼Œé€‚ç”¨äºä¸åŒæ€§èƒ½éœ€æ±‚çš„æœºå™¨ã€‚* 